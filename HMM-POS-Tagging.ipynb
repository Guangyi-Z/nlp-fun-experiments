{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Part-of-Speech Tagging\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "* v1.0: ignore sentences containing unknown words\n",
    "    * 21492/22470 = **0.9564753004005341**\n",
    "* v1.0: add sentences containing unknown words \n",
    "    * 21975/101136 = **0.21728168011390603**\n",
    "* v1.1: for unknown words, use only the best subsequent tag following tag_prev\n",
    "    * 93948/101136 = **0.9289273849074513**\n",
    "* v1.2: p(t|t_prev) in start or end place in a sentence\n",
    "    * 94016/101136 = **0.9295997468754944**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22723\n",
      "\n",
      "19980101-01-001-001/m  迈向/v  充满/v  希望/n  的/u  新/a  世纪/n  ——/w  一九九八年/t  新年/t  讲话/n  （/w  附/v  图片/n  １/m  张/q  ）/w  \r\n",
      "19980101-01-001-002/m  中共中央/nt  总书记/n  、/w  国家/n  主席/n  江/nr  泽民/nr  \r\n",
      "19980101-01-001-003/m  （/w  一九九七年/t  十二月/t  三十一日/t  ）/w  \r\n",
      "19980101-01-001-004/m  １２月/t  ３１日/t  ，/w  中共中央/nt  总书记/n  、/w  国家/n  主席/n  江/nr  泽民/nr  发表/v  １９９８年/t  新年/t  讲话/n  《/w  迈向/v  充满/v  希望/n  的/u  新/a  世纪/n  》/w  。/w  （/w  新华社/nt  记者/n  兰/nr  红光/nr  摄/Vg  ）/w  \r\n",
      "19980101-01-001-005/m  同胞/n  们/k  、/w  朋友/n  们/k  、/w  女士/n  们/k  、/w  先生/n  们/k  ：/w  \r\n",
      "19980101-01-001-006/m  在/p  １９９８年/t  来临/v  之际/f  ，/w  我/r  十分/m  高兴/a  地/u  通过/p  [中央/n  人民/n  广播/vn  电台/n]nt  、/w  [中国/ns  国际/n  广播/vn  电台/n]nt  和/c  [中央/n  电视台/n]nt  ，/w  向/p  全国/n  各族/r  人民/n  ，/w  向/p  [香港/ns  特别/a  行政区/n]ns  同胞/n  、/w  澳门/ns  和/c  台湾/ns  同胞/n  、/w  海外/s  侨胞/n  ，/w  向/p  世界/n  各国/r  的/u  朋友/n  们/k  ，/w  致以/v  诚挚/a  的/u  问候/vn  和/c  良好/a  的/u  祝愿/vn  ！/w  \r\n",
      "19980101-01-001-007/m  １９９７年/t  ，/w  是/v  中国/ns  发展/vn  历史/n  上/f  非常/d  重要/a  的/u  很/d  不/d  平凡/a  的/u  一/m  年/q  。/w  中国/ns  人民/n  决心/d  继承/v  邓/nr  小平/nr  同志/n  的/u  遗志/n  ，/w  继续/v  把/p  建设/v  有/v  中国/ns  特色/n  社会主义/n  事业/n  推向/v  前进/v  。/w  [中国/ns  政府/n]nt  顺利/ad  恢复/v  对/p  香港/ns  行使/v  主权/n  ，/w  并/c  按照/p  “/w  一国两制/j  ”/w  、/w  “/w  港人治港/l  ”/w  、/w  高度/d  自治/v  的/u  方针/n  保持/v  香港/ns  的/u  繁荣/an  稳定/an  。/w  [中国/ns  共产党/n]nt  成功/a  地/u  召开/v  了/u  第十五/m  次/q  全国/n  代表大会/n  ，/w  高举/v  邓小平理论/n  伟大/a  旗帜/n  ，/w  总结/v  百年/m  历史/n  ，/w  展望/v  新/a  的/u  世纪/n  ，/w  制定/v  了/u  中国/ns  跨/v  世纪/n  发展/v  的/u  行动/vn  纲领/n  。/w  \r\n",
      "19980101-01-001-008/m  在/p  这/r  一/m  年/q  中/f  ，/w  中国/ns  的/u  改革/vn  开放/vn  和/c  现代化/vn  建设/vn  继续/v  向前/v  迈进/v  。/w  国民经济/n  保持/v  了/u  “/w  高/a  增长/vn  、/w  低/a  通胀/j  ”/w  的/u  良好/a  发展/vn  态势/n  。/w  农业/n  生产/vn  再次/d  获得/v  好/a  的/u  收成/n  ，/w  企业/n  改革/vn  继续/v  深化/v  ，/w  人民/n  生活/vn  进一步/d  改善/v  。/w  对外/vn  经济/n  技术/n  合作/vn  与/c  交流/vn  不断/d  扩大/v  。/w  民主/a  法制/n  建设/vn  、/w  精神文明/n  建设/vn  和/c  其他/r  各项/r  事业/n  都/d  有/v  新/a  的/u  进展/vn  。/w  我们/r  十分/m  关注/v  最近/t  一个/m  时期/n  一些/m  国家/n  和/c  地区/n  发生/v  的/u  金融/n  风波/n  ，/w  我们/r  相信/v  通过/p  这些/r  国家/n  和/c  地区/n  的/u  努力/an  以及/c  有关/v  的/u  国际/n  合作/vn  ，/w  情况/n  会/v  逐步/d  得到/v  缓解/vn  。/w  总的来说/c  ，/w  中国/ns  改革/v  和/c  发展/v  的/u  全局/n  继续/v  保持/v  了/u  稳定/an  。/w  \r\n",
      "19980101-01-001-009/m  在/p  这/r  一/m  年/q  中/f  ，/w  中国/ns  的/u  外交/n  工作/vn  取得/v  了/u  重要/a  成果/n  。/w  通过/p  高层/n  互访/v  ，/w  中国/ns  与/p  美国/ns  、/w  俄罗斯/ns  、/w  法国/ns  、/w  日本/ns  等/u  大国/n  确定/v  了/u  双方/n  关系/n  未来/t  发展/v  的/u  目标/n  和/c  指导/vn  方针/n  。/w  中国/ns  与/p  周边/n  国家/n  和/c  广大/b  发展中国家/l  的/u  友好/a  合作/vn  进一步/d  加强/v  。/w  中国/ns  积极/ad  参与/v  [亚/j  太/j  经合/j  组织/n]nt  的/u  活动/vn  ，/w  参加/v  了/u  东盟/ns  —/w  中/j  日/j  韩/j  和/c  中国/ns  —/w  东盟/ns  首脑/n  非正式/b  会晤/vn  。/w  这些/r  外交/n  活动/vn  ，/w  符合/v  和平/n  与/c  发展/v  的/u  时代/n  主题/n  ，/w  顺应/v  世界/n  走向/v  多极化/v  的/u  趋势/n  ，/w  对于/p  促进/v  国际/n  社会/n  的/u  友好/a  合作/vn  和/c  共同/b  发展/vn  作出/v  了/u  积极/a  的/u  贡献/n  。/w  \r\n",
      "19980101-01-001-010/m  １９９８年/t  ，/w  中国/ns  人民/n  将/d  满怀信心/l  地/u  开创/v  新/a  的/u  业绩/n  。/w  尽管/c  我们/r  在/p  经济/n  社会/n  发展/v  中/f  还/d  面临/v  不少/m  困难/an  ，/w  但/c  我们/r  有/v  邓小平理论/n  的/u  指引/vn  ，/w  有/v  改革/v  开放/v  近/a  ２０/m  年/q  来/f  取得/v  的/u  伟大/a  成就/n  和/c  积累/v  的/u  丰富/a  经验/n  ，/w  还/d  有/v  其他/r  的/u  各种/r  有利/a  条件/n  ，/w  我们/r  一定/d  能够/v  克服/v  这些/r  困难/an  ，/w  继续/v  稳步前进/l  。/w  只要/c  我们/r  进一步/d  解放思想/i  ，/w  实事求是/i  ，/w  抓住/v  机遇/n  ，/w  开拓进取/l  ，/w  建设/v  有/v  中国/ns  特色/n  社会主义/n  的/u  道路/n  就/c  会/v  越/d  走/v  越/d  宽广/a  。/w  \r\n"
     ]
    }
   ],
   "source": [
    "# 人民日报\n",
    "# download corpus from the link below, and unzip it\n",
    "# http://icl.pku.edu.cn/icl_groups/corpus/dwldform1.asp\n",
    "\n",
    "f = '199801/199801.txt'\n",
    "with open(f, 'rb') as fin:\n",
    "    all = fin.read().decode('gbk').split('\\n')\n",
    "print(len(all), end='\\n\\n')\n",
    "\n",
    "for line in all[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "first-order HMM\n",
    "p(t1..tn, w1..wn) = p(w1..wn|t1..tn) * p(t1..tn)\n",
    "                  = p(w1|t1)*p(w2|t2)..p(wn|tn) * p(t1)*p(t2|t1)*p(t3|t2)                \n",
    "\n",
    "'''\n",
    "\n",
    "f = '199801/199801.txt'\n",
    "with open(f, 'rb') as fin:\n",
    "    all = fin.read().decode('gbk').split('\\n')\n",
    "\n",
    "fold = 0.9\n",
    "train_len = int(len(all) * fold)\n",
    "train_set = all[:train_len]\n",
    "test_set = all[train_len:]\n",
    "\n",
    "from collections import defaultdict # matrix as incursive dict\n",
    "prob_word_by_tag = dict() # emit matrix\n",
    "prob_tag_by_tag = dict()  # tran matrix\n",
    "\n",
    "for line in train_set:\n",
    "    tokens = line.split()[1:] # ignore 0th timestamp\n",
    "    prev_tag = None\n",
    "    for token in tokens:\n",
    "        if token.strip() == '':\n",
    "            continue\n",
    "            \n",
    "        word, tag = token.split('/')\n",
    "        # case: 大会堂/n]ns\n",
    "        if tag.find(']') != -1:\n",
    "            tag = tag.split(']')[0] # ignore the sub-tag\n",
    "        \n",
    "        if not prob_word_by_tag.get(tag, None):\n",
    "            prob_word_by_tag[tag] = defaultdict(lambda: -float('inf'))\n",
    "        prob_word_by_tag[tag][word] = prob_word_by_tag[tag].get(word, 0) + 1\n",
    "        \n",
    "        if prev_tag:\n",
    "            if not prob_tag_by_tag.get(prev_tag, None):\n",
    "                prob_tag_by_tag[prev_tag] = defaultdict(lambda: -float('inf'))\n",
    "            prob_tag_by_tag[prev_tag][tag] = prob_tag_by_tag[prev_tag].get(tag, 0) + 1\n",
    "        prev_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# use log instead of original probability, to avoid the chain product of probs converges into zero\n",
    "for tag,words_cnt in prob_word_by_tag.items():\n",
    "    ntotal = sum([c for w,c in words_cnt.items()])\n",
    "    for w,c in words_cnt.items():\n",
    "        words_cnt[w] = math.log(float(c) / ntotal)\n",
    "for prev_tag,tags_cnt in prob_tag_by_tag.items():\n",
    "    ntotal = sum([c for t,c in tags_cnt.items()])\n",
    "    for t,c in tags_cnt.items():\n",
    "        tags_cnt[t] = math.log(float(c) / ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.794260869027279\n",
      "-inf\n",
      "-1.678308670888908\n",
      "dict_keys(['Mg', 'r', 'Ng', 'p', 'd', 'l', 'c', 'vvn', 'ad', 'y', 'nz', 'vn', 'Bg', 'nx', 'h', 'm', 'i', 'f', 'nr', 'an', 'z', 'Rg', 'nt', 'Ag', 'k', 'vd', 'o', 'a', 'n', 'q', 'Dg', 'u', 'e', 'w', 'na', 'b', 'Yg', 'j', 't', 'Vg', 'ns', 's', 'Tg', 'v'])\n",
      "dict_keys(['Mg', 'r', 'Ng', 'p', 'd', 'l', 'c', 'vvn', 'ad', 'y', 'nz', 'vn', 'Bg', 'nx', 'h', 'm', 'i', 'f', 'nr', 'an', 'z', 'Rg', 'nt', 'Ag', 'k', 'vd', 'o', 'a', 'n', 'q', 'Dg', 'u', 'e', 'w', 'na', 'b', 'Yg', 'j', 't', 'Vg', 'ns', 's', 'Tg', 'v'])\n",
      "z=>i: -5.153291594497779\n",
      "z=>a: -3.4356400974234456\n",
      "z=>Ng: -6.406054562993147\n",
      "z=>p: -4.534252386091556\n",
      "z=>d: -4.208829985656927\n",
      "z=>y: -7.099201743553092\n",
      "z=>c: -4.208829985656927\n",
      "z=>u: -1.2214659617734531\n",
      "z=>l: -6.000589454884983\n",
      "z=>w: -1.661122434629897\n",
      "z=>vn: -5.307442274325037\n",
      "z=>n: -1.4970829226733913\n",
      "z=>b: -6.406054562993147\n",
      "z=>m: -3.4356400974234456\n",
      "z=>t: -7.099201743553092\n",
      "z=>Vg: -6.000589454884983\n",
      "z=>ns: -7.099201743553092\n",
      "z=>s: -5.712907382433202\n",
      "z=>r: -7.099201743553092\n",
      "z=>f: -5.489763831118992\n",
      "z=>nr: -7.099201743553092\n",
      "z=>v: -1.917418193261007\n",
      "z=>an: -6.406054562993147\n",
      "z=>z: -5.0197602018732566\n",
      "z=>Ag: -7.099201743553092\n"
     ]
    }
   ],
   "source": [
    "print(prob_word_by_tag['v']['改革'])\n",
    "print(prob_word_by_tag['v']['xxxx'])\n",
    "print(prob_tag_by_tag['v']['n'])\n",
    "print(prob_tag_by_tag.keys())\n",
    "print(prob_word_by_tag.keys())\n",
    "\n",
    "for t,p in prob_tag_by_tag['z'].items():\n",
    "    print('z=>{0}: {1}'.format(t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mg dict_keys(['a', 'n', 'v'])\n",
      "r dict_keys(['Ng', 'vd', 'o', 'r', 'a', 'ad', 'p', 'd', 'y', 'q', 'c', 'Dg', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'h', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'f', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "Ng dict_keys(['u', 'Ng', 'vd', 'r', 'a', 'f', 'p', 'd', 'l', 'q', 'c', 'Dg', 'ad', 'y', 'nz', 'w', 'vn', 'k', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'z', 'nt', 'Ag', 'Tg'])\n",
      "p dict_keys(['u', 'Ng', 'Mg', 'vd', 'o', 'r', 'a', 'ad', 'p', 'd', 'q', 'c', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'h', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'Rg', 'nt', 'Ag', 'Tg'])\n",
      "d dict_keys(['Ng', 'Mg', 'vd', 'o', 'i', 'a', 'ad', 'p', 'd', 'y', 'q', 'c', 'Dg', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'k', 'n', 'b', 'm', 'j', 't', 'Bg', 'Vg', 'ns', 's', 'r', 'f', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "l dict_keys(['ad', 'vd', 'r', 'a', 'Ng', 'p', 'd', 'y', 'q', 'c', 'u', 'l', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'f', 'nr', 'v', 'an', 'nt', 'Ag'])\n",
      "c dict_keys(['u', 'Ng', 'vd', 'r', 'a', 'ad', 'p', 'd', 'q', 'c', 'Dg', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'h', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "vvn dict_keys(['n'])\n",
      "ad dict_keys(['vn', 'u', 'n', 'm', 'i', 'a', 'j', 'ad', 'p', 'Vg', 'r', 'f', 'Tg', 'c', 'd', 'v', 'l', 't', 'w', 'vd'])\n",
      "y dict_keys(['n', 'Vg', 'Tg', 'q', 'y', 'f', 'w', 'd', 'v'])\n",
      "nz dict_keys(['Ng', 'ad', 'vd', 'r', 'a', 'f', 'p', 'd', 'q', 'c', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'k', 'n', 'b', 'h', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt'])\n",
      "vn dict_keys(['Ng', 'ad', 'vd', 'i', 'a', 'f', 'p', 'd', 'y', 'q', 'c', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'k', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'r', 'nr', 'v', 'an', 'z', 'Tg'])\n",
      "Bg dict_keys(['a', 'n', 'c', 'w', 'u'])\n",
      "nx dict_keys(['vn', 'k', 'n', 'Ng', 'a', 'v', 'r', 'm', 'f', 'p', 'b', 'd', 's', 'i', 'q', 'c', 'nr', 'u', 'ad', 'nz', 'w'])\n",
      "h dict_keys(['vn', 'n', 'b', 'm', 'v', 'a', 'j', 'nt', 'nz', 'ns'])\n",
      "m dict_keys(['k', 'ad', 'vd', 'o', 'r', 'a', 'Ng', 'p', 'd', 'y', 'n', 'c', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'q', 'b', 'm', 'j', 't', 'Bg', 'Vg', 'ns', 's', 'i', 'f', 'Tg', 'v', 'an', 'z', 'Ag', 'nr'])\n",
      "i dict_keys(['k', 'ad', 'i', 'a', 'Ng', 'p', 'd', 'l', 'q', 'c', 'Dg', 'u', 'y', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'r', 'f', 'nr', 'v', 'z', 'Ag', 'Tg'])\n",
      "f dict_keys(['u', 'Ng', 'vd', 'o', 'r', 'a', 'f', 'p', 'd', 'y', 'q', 'c', 'Dg', 'ad', 'l', 'nz', 'w', 'vn', 'k', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ag'])\n",
      "nr dict_keys(['Ng', 'vd', 'q', 'i', 'a', 'ad', 'p', 'd', 'l', 'k', 'c', 'Dg', 'u', 'y', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'r', 'f', 'nr', 'o', 'v', 'z', 'nt', 'Tg'])\n",
      "an dict_keys(['ad', 'i', 'a', 'Ng', 'p', 'd', 'y', 'q', 'c', 'u', 'l', 'w', 'vn', 'n', 'm', 'ns', 'r', 'f', 'nr', 'v', 'an', 'z', 'nt', 'Tg'])\n",
      "z dict_keys(['i', 'a', 'Ng', 'p', 'd', 'y', 'c', 'u', 'l', 'w', 'vn', 'n', 'b', 'm', 't', 'Vg', 'ns', 's', 'r', 'f', 'nr', 'v', 'an', 'z', 'Ag'])\n",
      "Rg dict_keys(['k', 'u', 'Rg', 'Ng', 'w', 'd'])\n",
      "nt dict_keys(['ad', 'r', 'a', 'Ng', 'p', 'd', 'c', 'u', 'l', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'f', 'nr', 'v', 'an', 'nt'])\n",
      "Ag dict_keys(['vn', 'u', 'n', 'b', 'a', 'v', 'r', 'm', 'Ng', 'p', 'Vg', 'ns', 'q', 'y', 'Bg', 'c', 'd', 'f', 'l', 'w', 'nr'])\n",
      "k dict_keys(['u', 'Ng', 'i', 'a', 'f', 'p', 'd', 'y', 'c', 'ad', 'l', 'b', 'w', 'vn', 'n', 'nx', 'm', 't', 's', 'r', 'nr', 'v'])\n",
      "vd dict_keys(['vn', 'u', 'n', 'vd', 'v', 'a', 'ad', 'p', 'd', 'Vg', 'f', 'l', 'w'])\n",
      "o dict_keys(['n', 'a', 'v', 'm', 'u', 'w', 'd'])\n",
      "a dict_keys(['u', 'k', 'ad', 'vd', 'r', 'a', 'Ng', 'p', 'd', 'y', 'q', 'c', 'Dg', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "n dict_keys(['u', 'k', 'Ng', 'vd', 'o', 'r', 'a', 'f', 'p', 'd', 'y', 'q', 'c', 'Dg', 'ad', 'l', 'nx', 'nz', 'w', 'vn', 'e', 'n', 'b', 'h', 'm', 'j', 't', 'Bg', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'Rg', 'nt', 'Ag', 'Tg'])\n",
      "q dict_keys(['Ng', 'ad', 'vd', 'r', 'a', 'u', 'p', 'd', 'y', 'q', 'c', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'h', 'm', 'j', 't', 'Bg', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "Dg dict_keys(['n', 'a', 'r', 'm', 't', 'p', 'd', 'w', 'Vg', 'v', 'l', 'Ag'])\n",
      "u dict_keys(['u', 'Yg', 'ad', 'vd', 'o', 'r', 'a', 'Ng', 'p', 'd', 'na', 'y', 'q', 'c', 'Dg', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'h', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "e dict_keys(['e', 'u', 'w', 'v'])\n",
      "w dict_keys(['Ng', 'Mg', 'vd', 'o', 'r', 'a', 'e', 'ad', 'p', 'd', 'y', 'q', 'c', 'Dg', 'vvn', 'u', 'l', 'b', 'nz', 'w', 'vn', 'k', 'n', 'nx', 'h', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'f', 'nr', 'v', 'an', 'z', 'Rg', 'nt', 'Ag', 'Tg'])\n",
      "na dict_keys(['w'])\n",
      "b dict_keys(['Ng', 'ad', 'r', 'a', 'f', 'p', 'd', 'y', 'q', 'c', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ag'])\n",
      "Yg dict_keys(['w'])\n",
      "j dict_keys(['k', 'ad', 'vd', 'r', 'a', 'u', 'p', 'd', 'y', 'q', 'c', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'nt', 'Ng', 'Tg'])\n",
      "t dict_keys(['u', 'ad', 'vd', 'i', 'a', 'Ng', 'p', 'd', 'y', 'q', 'c', 'f', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'r', 'Tg', 'v', 'an', 'z', 'nt', 'Ag', 'nr'])\n",
      "Vg dict_keys(['k', 'ad', 'o', 'r', 'a', 'Ng', 'p', 'd', 'y', 'q', 'c', 'u', 'l', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'f', 'nr', 'v', 'an', 'z', 'nt', 'Ag', 'Tg'])\n",
      "ns dict_keys(['Ng', 'vd', 'r', 'a', 'ad', 'p', 'd', 'y', 'q', 'c', 'u', 'l', 'nx', 'nz', 'w', 'vn', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'f', 'Tg', 'v', 'an', 'z', 'nt', 'Ag', 'nr'])\n",
      "s dict_keys(['u', 'Ng', 'vd', 'o', 'r', 'a', 'f', 'p', 'd', 'y', 'q', 'c', 'Dg', 'ad', 'l', 'nx', 'nz', 'w', 'vn', 'e', 'n', 'b', 'm', 'j', 't', 'Vg', 'ns', 's', 'i', 'Tg', 'v', 'an', 'z', 'nr'])\n",
      "Tg dict_keys(['ad', 'r', 'a', 'Ng', 'p', 'd', 'l', 'q', 'c', 'u', 'y', 'nz', 'w', 'vn', 'n', 'm', 'j', 't', 'Vg', 'ns', 'f', 'Tg', 'v', 'z', 'nr'])\n",
      "v dict_keys(['u', 'Ng', 'Mg', 'vd', 'o', 'r', 'a', 'f', 'p', 'd', 'y', 'q', 'c', 'Dg', 'ad', 'l', 'nx', 'nz', 'w', 'vn', 'k', 'n', 'b', 'h', 'm', 'j', 't', 'Bg', 'Vg', 'ns', 's', 'i', 'nr', 'v', 'an', 'z', 'Rg', 'nt', 'Ag', 'Tg'])\n"
     ]
    }
   ],
   "source": [
    "set1 = set()\n",
    "for t1,d in prob_tag_by_tag.items():\n",
    "    print(t1, d.keys())\n",
    "    set1 = set1.union(d.keys())\n",
    "set2 = set(prob_tag_by_tag.keys())\n",
    "\n",
    "assert len(set1) == len(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words: 53635\n",
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'v', 'n', 'nr', 'nr', 'u', 'v']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'vn', 'n', 'nr', 'nr', 'u', 'v']\n",
      "['nt', 'ns', 't', 't', 'n', 'w', 'n', 'n', 'vn', 'n', 'n', 'nr', 'nr', 'w', 'nt', 'n', 'nr', 'nr', 'w', 'p', 'ns', 'n', 'u', 'n', 'n', 't', 'n', 'd', 'l', 'w', 'nt', 'n', 't', 't', 'p', 'ns', 'nt', 'a', 'n', 'v', 't', 'n', 'w']\n",
      "['nt', 'ns', 't', 't', 'n', 'w', 'n', 'n', 'vn', 'n', 'n', 'nr', 'nr', 'w', 'nt', 'n', 'nr', 'nr', 'w', 'p', 'ns', 'n', 'u', 'n', 'n', 't', 'n', 'd', 'l', 'w', 'nt', 'n', 't', 't', 'p', 'ns', 'nt', 'a', 'n', 'v', 't', 'n', 'w']\n",
      "['r', 'v', 'w', 'p', 'v', 'u', 'm', 'q', 'f', 'w', 'n', 'v', 'v', 'c', 'n', 'vn', 'n', 'v', 'u', 'a', 'u', 'n', 'w', 'v', 'u', 'a', 'vn', 'w', 'a', 'j', 'w', 'n', 'l', 'w', 'v', 'u', 'a', 'u', 'vn', 'n', 'w']\n",
      "['r', 'v', 'w', 'p', 't', 'u', 'm', 'q', 'f', 'w', 'n', 'vn', 'vn', 'c', 'n', 'vn', 'n', 'v', 'u', 'a', 'u', 'n', 'w', 'v', 'u', 'a', 'vn', 'w', 'a', 'j', 'w', 'n', 'l', 'w', 'v', 'u', 'a', 'u', 'vn', 'n', 'w']\n",
      "['nr', 'nr', 'v', 'w', 'nt', 'n', 'u', 'n', 'k', 'w', 'ad', 'v', 'w', 'd', 'v', 'w', 'd', 'a', 'u', 'v', 'u', 'n', 'n', 'n', 'w', 'r', 'v', 'w', 'a', 'ns', 'v', 'f', 'w', 'nr', 'nr', 'n', 'd', 'v', 'nt', 'u', 'vn', 'w', 'v', 'c', 'v', 'u', 'z', 'u', 'n', 'c', 'n', 'vn', 'n', 'w', 'p', 'v', 'v', 'c', 'vn', 'vn', 'u', 'a', 'n', 'f', 'w', 'v', 'v', 'v', 'r', 'n', 'c', 'n', 'w', 'p', 'vn', 'v', 'a', 'w']\n",
      "['nr', 'nr', 'v', 'w', 'nt', 'n', 'u', 'n', 'k', 'w', 'a', 'vn', 'w', 'd', 'v', 'w', 'd', 'a', 'u', 'v', 'u', 'n', 'n', 'n', 'w', 'r', 'v', 'w', 'a', 'ns', 'v', 'f', 'w', 'nr', 'nr', 'n', 'd', 'v', 'nt', 'u', 'vn', 'w', 'v', 'c', 'v', 'u', 'z', 'u', 'n', 'c', 'n', 'vn', 'n', 'w', 'p', 'vn', 'vn', 'c', 'vn', 'vn', 'u', 'a', 'n', 'f', 'w', 'v', 'v', 'v', 'r', 'n', 'c', 'n', 'w', 'p', 'vn', 'v', 'a', 'w']\n",
      "['nr', 'nr', 'v', 'w', 'v', 'a', 'nt', 'u', 'vn', 'd', 'a', 'u', 'm', 'q', 'd', 'v', 'v', 'p', 'nt', 'v', 'an', 'w', 'p', 'nt', 'u', 'vn', 'f', 'w', 'v', 'a', 'r', 'n', 'u', 'n', 'w', 'd', 'v', 'v', 'v', 'a', 'v', 'w', 'v', 'c', 'v', 'u', 'n', 'w', 'ad', 'v', 'n', 'w', 'l', 'w', 'v', 'a', 'r', 'n', 'n', 'c', 'r', 'n', 'u', 'n', 'w', 'r', 'v', 'w', 'p', 'nt', 'n', 'n', 'v', 'd', 'a', 'w', 'c', 'n', 'k', 'd', 'v', 'v', 'p', 'ns', 'v', 'u', 'r', 'q', 'n', 'w', 'd', 'a', 'u', 'p', 'n', 'v', 'w']\n",
      "['nr', 'nr', 'v', 'w', 'v', 'a', 'n', 'u', 'vn', 'd', 'a', 'u', 'm', 'q', 'd', 'v', 'v', 'p', 'nt', 'v', 'a', 'w', 'p', 'nt', 'u', 'n', 'f', 'w', 'v', 'a', 'r', 'n', 'u', 'n', 'w', 'd', 'v', 'v', 'v', 'a', 'vn', 'w', 'v', 'p', 'a', 'u', 'n', 'w', 'ad', 'v', 'n', 'w', 'l', 'w', 'p', 'a', 'r', 'n', 'n', 'c', 'r', 'n', 'u', 'n', 'w', 'r', 'v', 'w', 'p', 'nt', 'n', 'n', 'vn', 'd', 'a', 'w', 'c', 'n', 'k', 'd', 'v', 'v', 'p', 'ns', 'vn', 'u', 'r', 'q', 'n', 'w', 'd', 'a', 'u', 'v', 'n', 'vn', 'w']\n",
      "['nr', 'nr', 'v', 'w', 'p', 'a', 'u', 'm', 'q', 'f', 'w', 'v', 'v', 'c', 'vn', 'vn', 'u', 'n', 'd', 'm', 'a', 'w', 'd', 'v', 'm', 'an', 'c', 'n', 'v', 'r', 'v', 'v', 'c', 'v', 'w', 'c', 'r', 'd', 'a', 'u', 'v', 'p', 'p', 'nr', 'nr', 'n', 'v', 'n', 'u', 'nt', 'f', 'w', 'p', 'j', 'n', 'u', 'vn', 'f', 'w', 'l', 'w', 'v', 'an', 'w', 'd', 'd', 'v', 'v', 'd', 'a', 'u', 'n', 'w']\n",
      "['nr', 'nr', 'v', 'w', 'p', 'a', 'u', 'm', 'q', 'f', 'w', 'vn', 'vn', 'c', 'vn', 'vn', 'u', 'n', 'd', 'm', 'a', 'w', 'd', 'v', 'm', 'an', 'c', 'n', 'v', 'r', 'v', 'v', 'c', 'v', 'w', 'c', 'r', 'd', 'a', 'u', 'a', 'p', 'p', 'nr', 'nr', 'n', 'p', 'n', 'u', 'nt', 'f', 'w', 'p', 'j', 'n', 'u', 'vn', 'f', 'w', 'l', 'w', 'v', 'an', 'w', 'd', 'd', 'v', 'v', 'd', 'a', 'u', 'n', 'w']\n",
      "['v', 'n', 'u', 'nt', 'n', 'n', 'v', 'w', 'nt', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'w', 'nr', 'nr', 'u', 'w']\n",
      "['v', 'n', 'u', 'nt', 'n', 'n', 'v', 'w', 'nt', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'w', 'nr', 'nr', 'u', 'w']\n",
      "21492/22470=0.9564753004005341\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging v1.0: ignore sentences containing unknown words \n",
    "\n",
    "def tagging(words, all_tags):\n",
    "    N = len(words)\n",
    "    mat = list()\n",
    "    for _ in range(N):\n",
    "        mat.append(defaultdict(float))\n",
    "        \n",
    "    back_trace = defaultdict(list)\n",
    "    for t in all_tags:\n",
    "        mat[0][t] = prob_word_by_tag[t][words[0]]\n",
    "        back_trace[t] = [t]\n",
    "        \n",
    "    for idx,w in enumerate(words[1:]):\n",
    "        idx += 1\n",
    "        new_back_trace = defaultdict(list)\n",
    "        for t in all_tags:\n",
    "            # trick: -float('inf') + 1 == -float('inf') + 2\n",
    "            score, pt = max((s+prob_word_by_tag[t][w]+prob_tag_by_tag[pt][t],pt) for pt,s in mat[idx-1].items())\n",
    "            if score == -float('inf'):\n",
    "                pass # unknown words\n",
    "                \n",
    "            mat[idx][t] = score\n",
    "            new_back_trace[t] = back_trace[pt] + [t]\n",
    "        back_trace = new_back_trace\n",
    "        \n",
    "    max_score, final_tag = max((s,t) for t,s in mat[N-1].items())\n",
    "    \n",
    "    return max_score, back_trace[final_tag]\n",
    "    \n",
    "\n",
    "all_words = set()\n",
    "for t,words_cnt in prob_word_by_tag.items():\n",
    "    all_words = all_words.union(words_cnt.keys())\n",
    "print('all_words: {0}'.format(len(all_words)))\n",
    "    \n",
    "num_matched_total = 0\n",
    "num_total = 0\n",
    "\n",
    "cnt=10\n",
    "for line in test_set:\n",
    "    tokens = line.split()[1:] # ignore 0th timestamp\n",
    "    if not tokens:\n",
    "        continue\n",
    "    words, tags = zip(*[tk.split('/') for tk in tokens])\n",
    "    tags = list(map(lambda t: t.split(']')[0] if t.find(']')>=0 else t, tags)) # case: 大会堂/n]ns, ignore the sub-tag\n",
    "    assert len(words) == len(tags)\n",
    "    \n",
    "    # ignore sentence containing unknown words\n",
    "    has_unknown_word = False\n",
    "    for w in words:\n",
    "        if w not in all_words:\n",
    "            has_unknown_word = True\n",
    "    if has_unknown_word:\n",
    "        continue\n",
    "            \n",
    "    max_score, tags_predict = tagging(words, prob_tag_by_tag.keys())\n",
    "    if cnt>0:\n",
    "        print(tags)\n",
    "        cnt -= 1\n",
    "        print(tags_predict)\n",
    "    assert len(tags) == len(tags_predict)\n",
    "    \n",
    "    num_matched_total += len(list(filter(lambda x: x, [t1==t2 for t1,t2 in zip(tags,tags_predict)])))\n",
    "    num_total += len(tags)\n",
    "\n",
    "print('{0}/{1}={2}'.format(num_matched_total, num_total, float(num_matched_total)/num_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['r', 'v', 'ns', 'n', 'i', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'v', 'n', 'a', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'u', 'n', 'n', 'w', 'ad', 'v', 'ns', 'n', 'v', 'u', 'n', 'w', 'c', 'c', 'n', 'w', 'j', 'n', 'w', 'c', 'n', 'w', 'n', 'w', 'n', 'n', 'w', 'n', 'w', 'c', 'c', 'v', 'p', 'ns', 'u', 'r', 'n', 'n', 'c', 'f', 'n', 'n', 'w', 'c', 'v', 'p', 'ns', 'w', 'ns', 'c', 's', 'u', 'ns', 'n', 'w', 'c', 'c', 'v', 'v', 'n', 'n', 'c', 'n', 'vn', 'u', 'n', 'w', 'c', 'v', 'v', 'w', 'v', 'c', 'n', 'n', 'd', 'v', 'a', 'w', 'vn', 'n', 'u', 'n', 'w', 'r', 'd', 'd', 'p', 'r', 'ad', 'v', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 't', 'p', 'w', 'j', 'w', 'u', 'n', 'v', 'n', 'n', 'vn', 'w', 'p', 'ns', 'n', 'u', 'a', 'n', 'c', 'a', 'n', 'v', 'r', 'vn', 'w', 'v', 'p', 'ns', 'p', 'w', 'j', 'w', 'u', 'n', 'f', 'v', 'an', 'an', 'w', 'd', 'a', 'u', 'ns', 'n', 'd', 'd', 'v', 'p', 'w', 'j', 'w', 'u', 'vn', 'c', 'vn', 'w']\n",
      "['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "['nr', 'nr', 'f', 'v', 'w', 'n', 'n', 'vn', 'v', 'v', 'v', 'u', 'n', 'w', 'ns', 'n', 'd', 'v', 'v', 'vn', 'w', 'n', 'vn', 'd', 'v', 'v', 'w', 'r', 'p', 'r', 'l', 'f', 'w', 'p', 'b', 'ns', 'n', 'l', 'w']\n",
      "['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "['nt', 'ns', 'vn', 'n', 'w', 'nt', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'p', 'vn', 'f', 'v', 'w', 'nr', 'n', 'p', 'a', 'n', 'f', 'v', 'c', 'p', 'j', 'n', 'f', 'v', 'u', 'p', 'v', 'w', 'p', 'm', 'ns', 'u', 'n', 'f', 'w', 'ad', 'v', 'n', 'a', 'n', 'w', 'vn', 'u', 'a', 'n', 'w', 'ad', 'v', 'u', 'r', 'n', 'c', 'n', 'v', 'n', 'n', 'vn', 'u', 'n', 'w', 'v', 'n', 'n', 'u', 'a', 'vn', 'w', 'c', 'ns', 'n', 'ad', 'v', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'vn', 'u', 'n', 'w', 'nt', 'j', 'w', 'nt', 'j', 'd', 'v', 'v', 'n', 'n', 'n', 'n', 'p', 'n', 'v', 'n', 'vn', 'u', 'n', 'vn', 'w', 'v', 'ns', 'n', 'ad', 'v', 'r', 'u', 'n', 'c', 'n', 'w', 'd', 'v', 'u', 'd', 'p', 'r', 'v', 'n', 'vn', 'w', 'p', 'v', 'n', 'n', 'v', 'a', 'u', 'vn', 'w']\n",
      "['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "['ns', 'an', 'n', 'n', 'nr', 'nr', 'v', 'r', 'n', 'w', 'n', 'j', 'c', 'l', 'w', 'nz', 'n', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'v', 'ns', 'an', 'vn', 'n', 'w', 'n', 'j', 'w', 'n', 'n', 'n', 'n', 'w', 'n', 'n', 'n', 'm', 'n', 'nr', 'nr', 'v', 'n', 'n', 'w', 'n', 'n', 'c', 'n', 'j', 'd', 'p', 't', 'v', 'w', 'r', 'v', 'w', 'nr', 'n', 'v', 'u', 'vn', 'n', 'n', 'w', 'v', 'n', 'n', 'vn', 'n', 'u', 'm', 'q', 'n', 'w', 'd', 'v', 'u', 'n', 'u', 'a', 'n', 'w', 'v', 'u', 's', 'ns', 'n', 'u', 'b', 'n', 'w', 'v', 'v', 'n', 'vn', 'u', 'n', 'c', 'n', 'd', 'v', 'u', 'l', 'u', 'n', 'w', 'r', 'n', 'w', 'n', 'j', 'w', 'r', 'l', 'c', 'l', 'ad', 'v', 'nr', 'n', 'vn', 'n', 'd', 'v', 'n', 'vn', 'u', 'n', 'w', 'v', 'ns', 'n', 'i', 'w', 'p', 'n', 'n', 'v', 'w', 'v', 'a', 'n', 'w', 'ad', 'v', 'ns', 'n', 'p', 'n', 'n', 'vn', 'u', 'a', 'vn', 'w', 'ad', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'n', 'vn', 'u', 'n', 'w']\n",
      "['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['nt', 'w', 'nt', 'vn', 'n', 'w', 'n', 'n', 'c', 'ns', 'vn', 'vn', 'n', 'w', 'p', 'j', 'n', 'w', 'n', 'n', 'd', 'm', 'n', 'v', 'u', 'n', 'w']\n",
      "['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "['n', 'p', 'j', 'w', 'j', 'w', 'ns', 'an', 'vn', 'n', 'w', 'ns', 'n', 'vn', 'n', 'w', 'n', 'j', 'j', 'vn', 'n', 'w', 'nz', 'n', 'ns', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'd', 'v', 'w']\n",
      "['z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z', 'z']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'v', 'n', 'nr', 'nr', 'u', 'v']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'vn', 'n', 'nr', 'nr', 'u', 'v']\n",
      "21975/101136=0.21728168011390603\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging v1.0: add sentences containing unknown words \n",
    "\n",
    "def tagging(words, all_tags):\n",
    "    N = len(words)\n",
    "    mat = list()\n",
    "    for _ in range(N):\n",
    "        mat.append(defaultdict(float))\n",
    "        \n",
    "    back_trace = defaultdict(list)\n",
    "    for t in all_tags:\n",
    "        mat[0][t] = prob_word_by_tag[t][words[0]]\n",
    "        back_trace[t] = [t]\n",
    "        \n",
    "    for idx,w in enumerate(words[1:]):\n",
    "        idx += 1\n",
    "        new_back_trace = defaultdict(list)\n",
    "        for t in all_tags:\n",
    "            # trick: -float('inf') + 1 == -float('inf') + 2\n",
    "            score, pt = max((s+prob_word_by_tag[t][w]+prob_tag_by_tag[pt][t],pt) for pt,s in mat[idx-1].items())\n",
    "            if score == -float('inf'):\n",
    "                pass # unknown words\n",
    "                \n",
    "            mat[idx][t] = score\n",
    "            new_back_trace[t] = back_trace[pt] + [t]\n",
    "        back_trace = new_back_trace\n",
    "        \n",
    "    max_score, final_tag = max((s,t) for t,s in mat[N-1].items())\n",
    "    \n",
    "    return max_score, back_trace[final_tag]\n",
    "    \n",
    "\n",
    "num_matched_total = 0\n",
    "num_total = 0\n",
    "\n",
    "cnt=10\n",
    "for line in test_set:\n",
    "    tokens = line.split()[1:] # ignore 0th timestamp\n",
    "    if not tokens:\n",
    "        continue\n",
    "    words, tags = zip(*[tk.split('/') for tk in tokens])\n",
    "    tags = list(map(lambda t: t.split(']')[0] if t.find(']')>=0 else t, tags)) # case: 大会堂/n]ns, ignore the sub-tag\n",
    "    assert len(words) == len(tags)\n",
    "            \n",
    "    max_score, tags_predict = tagging(words, prob_tag_by_tag.keys())\n",
    "    if cnt>0:\n",
    "        print(tags)\n",
    "        cnt -= 1\n",
    "        print(tags_predict)\n",
    "    assert len(tags) == len(tags_predict)\n",
    "    \n",
    "    num_matched_total += len(list(filter(lambda x: x, [t1==t2 for t1,t2 in zip(tags,tags_predict)])))\n",
    "    num_total += len(tags)\n",
    "\n",
    "print('{0}/{1}={2}'.format(num_matched_total, num_total, float(num_matched_total)/num_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['r', 'v', 'ns', 'n', 'i', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'v', 'n', 'a', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'u', 'n', 'n', 'w', 'ad', 'v', 'ns', 'n', 'v', 'u', 'n', 'w', 'c', 'c', 'n', 'w', 'j', 'n', 'w', 'c', 'n', 'w', 'n', 'w', 'n', 'n', 'w', 'n', 'w', 'c', 'c', 'v', 'p', 'ns', 'u', 'r', 'n', 'n', 'c', 'f', 'n', 'n', 'w', 'c', 'v', 'p', 'ns', 'w', 'ns', 'c', 's', 'u', 'ns', 'n', 'w', 'c', 'c', 'v', 'v', 'n', 'n', 'c', 'n', 'vn', 'u', 'n', 'w', 'c', 'v', 'v', 'w', 'v', 'c', 'n', 'n', 'd', 'v', 'a', 'w', 'vn', 'n', 'u', 'n', 'w', 'r', 'd', 'd', 'p', 'r', 'ad', 'v', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 't', 'p', 'w', 'j', 'w', 'u', 'n', 'v', 'n', 'n', 'vn', 'w', 'p', 'ns', 'n', 'u', 'a', 'n', 'c', 'a', 'n', 'v', 'r', 'vn', 'w', 'v', 'p', 'ns', 'p', 'w', 'j', 'w', 'u', 'n', 'f', 'v', 'an', 'an', 'w', 'd', 'a', 'u', 'ns', 'n', 'd', 'd', 'v', 'p', 'w', 'j', 'w', 'u', 'vn', 'c', 'vn', 'w']\n",
      "['r', 'v', 'ns', 'n', 'i', 'u', 'n', 'w', 'vn', 'c', 'v', 'ns', 'n', 'vn', 'n', 'a', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'u', 'n', 'n', 'w', 'ad', 'v', 'ns', 'n', 'v', 'u', 'n', 'w', 'c', 'v', 'n', 'w', 'j', 'n', 'w', 'c', 'n', 'w', 'n', 'w', 'n', 'n', 'w', 'n', 'w', 'c', 'v', 'v', 'p', 'ns', 'u', 'r', 'v', 'n', 'c', 'f', 'w', 'n', 'w', 'c', 'v', 'p', 'ns', 'w', 'ns', 'c', 's', 'u', 'ns', 'n', 'w', 'c', 'v', 'n', 'vn', 'n', 'n', 'c', 'n', 'v', 'u', 'n', 'w', 'c', 'v', 'v', 'w', 'vn', 'c', 'n', 'n', 'd', 'v', 'a', 'w', 'vn', 'n', 'u', 'n', 'w', 'r', 'd', 'd', 'p', 'r', 'ad', 'v', 'w', 'v', 'vn', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 't', 'p', 'w', 'j', 'w', 'u', 'n', 'v', 'n', 'n', 'vn', 'w', 'p', 'ns', 'n', 'u', 'n', 'n', 'c', 'a', 'n', 'v', 'r', 'v', 'w', 'v', 'p', 'ns', 'p', 'w', 'j', 'w', 'u', 'n', 'f', 'v', 'an', 'an', 'w', 'd', 'a', 'u', 'ns', 'n', 'd', 'd', 'v', 'p', 'w', 'j', 'w', 'u', 'vn', 'c', 'vn', 'w']\n",
      "['nr', 'nr', 'f', 'v', 'w', 'n', 'n', 'vn', 'v', 'v', 'v', 'u', 'n', 'w', 'ns', 'n', 'd', 'v', 'v', 'vn', 'w', 'n', 'vn', 'd', 'v', 'v', 'w', 'r', 'p', 'r', 'l', 'f', 'w', 'p', 'b', 'ns', 'n', 'l', 'w']\n",
      "['nr', 'nr', 'f', 'v', 'w', 'n', 'n', 'v', 'v', 'v', 'v', 'u', 'n', 'w', 'ns', 'n', 'd', 'v', 'v', 'v', 'w', 'n', 'vn', 'd', 'v', 'v', 'w', 'r', 'p', 'r', 'l', 'f', 'w', 'p', 'b', 'ns', 'n', 'n', 'w']\n",
      "['nt', 'ns', 'vn', 'n', 'w', 'nt', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'p', 'vn', 'f', 'v', 'w', 'nr', 'n', 'p', 'a', 'n', 'f', 'v', 'c', 'p', 'j', 'n', 'f', 'v', 'u', 'p', 'v', 'w', 'p', 'm', 'ns', 'u', 'n', 'f', 'w', 'ad', 'v', 'n', 'a', 'n', 'w', 'vn', 'u', 'a', 'n', 'w', 'ad', 'v', 'u', 'r', 'n', 'c', 'n', 'v', 'n', 'n', 'vn', 'u', 'n', 'w', 'v', 'n', 'n', 'u', 'a', 'vn', 'w', 'c', 'ns', 'n', 'ad', 'v', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'vn', 'u', 'n', 'w', 'nt', 'j', 'w', 'nt', 'j', 'd', 'v', 'v', 'n', 'n', 'n', 'n', 'p', 'n', 'v', 'n', 'vn', 'u', 'n', 'vn', 'w', 'v', 'ns', 'n', 'ad', 'v', 'r', 'u', 'n', 'c', 'n', 'w', 'd', 'v', 'u', 'd', 'p', 'r', 'v', 'n', 'vn', 'w', 'p', 'v', 'n', 'n', 'v', 'a', 'u', 'vn', 'w']\n",
      "['nt', 'ns', 'vn', 'n', 'w', 'nt', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'p', 'vn', 'f', 'v', 'w', 'nr', 'n', 'p', 'a', 'n', 'f', 'v', 'c', 'p', 'j', 'n', 'f', 'v', 'u', 'p', 'v', 'w', 'p', 'm', 'ns', 'u', 'n', 'f', 'w', 'ad', 'v', 'n', 'a', 'n', 'w', 'v', 'u', 'a', 'n', 'w', 'ad', 'v', 'u', 'r', 'n', 'c', 'n', 'v', 'n', 'n', 'vn', 'u', 'n', 'w', 'v', 'n', 'n', 'u', 'a', 'vn', 'w', 'c', 'ns', 'n', 'ad', 'v', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'vn', 'u', 'n', 'w', 'nt', 'j', 'w', 'nt', 'j', 'd', 'v', 'v', 'n', 'n', 'n', 'n', 'c', 'v', 'v', 'n', 'vn', 'u', 'n', 'vn', 'w', 'v', 'ns', 'n', 'ad', 'v', 'r', 'u', 'n', 'c', 'n', 'w', 'd', 'v', 'u', 'd', 'p', 'r', 'v', 'n', 'vn', 'w', 'p', 'v', 'n', 'n', 'v', 'a', 'u', 'vn', 'w']\n",
      "['ns', 'an', 'n', 'n', 'nr', 'nr', 'v', 'r', 'n', 'w', 'n', 'j', 'c', 'l', 'w', 'nz', 'n', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'v', 'ns', 'an', 'vn', 'n', 'w', 'n', 'j', 'w', 'n', 'n', 'n', 'n', 'w', 'n', 'n', 'n', 'm', 'n', 'nr', 'nr', 'v', 'n', 'n', 'w', 'n', 'n', 'c', 'n', 'j', 'd', 'p', 't', 'v', 'w', 'r', 'v', 'w', 'nr', 'n', 'v', 'u', 'vn', 'n', 'n', 'w', 'v', 'n', 'n', 'vn', 'n', 'u', 'm', 'q', 'n', 'w', 'd', 'v', 'u', 'n', 'u', 'a', 'n', 'w', 'v', 'u', 's', 'ns', 'n', 'u', 'b', 'n', 'w', 'v', 'v', 'n', 'vn', 'u', 'n', 'c', 'n', 'd', 'v', 'u', 'l', 'u', 'n', 'w', 'r', 'n', 'w', 'n', 'j', 'w', 'r', 'l', 'c', 'l', 'ad', 'v', 'nr', 'n', 'vn', 'n', 'd', 'v', 'n', 'vn', 'u', 'n', 'w', 'v', 'ns', 'n', 'i', 'w', 'p', 'n', 'n', 'v', 'w', 'v', 'a', 'n', 'w', 'ad', 'v', 'ns', 'n', 'p', 'n', 'n', 'vn', 'u', 'a', 'vn', 'w', 'ad', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'n', 'vn', 'u', 'n', 'w']\n",
      "['ns', 'a', 'n', 'n', 'nr', 'nr', 'v', 'r', 'n', 'w', 'n', 'j', 'c', 'l', 'w', 'nz', 'n', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'v', 'ns', 'a', 'vn', 'n', 'w', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'w', 'n', 'n', 'n', 'm', 'n', 'nr', 'nr', 'n', 'n', 'n', 'w', 'n', 'n', 'c', 'n', 'j', 'd', 'p', 't', 'v', 'w', 'r', 'v', 'w', 'nr', 'n', 'v', 'u', 'vn', 'n', 'n', 'w', 'v', 'n', 'n', 'vn', 'n', 'u', 'm', 'q', 'n', 'w', 'd', 'v', 'u', 'n', 'u', 'a', 'n', 'w', 'v', 'u', 's', 'ns', 'n', 'u', 'b', 'n', 'w', 'v', 'v', 'n', 'v', 'u', 'n', 'c', 'n', 'd', 'a', 'u', 'l', 'u', 'n', 'w', 'r', 'n', 'w', 'n', 'j', 'w', 'r', 'l', 'c', 'l', 'ad', 'v', 'nr', 'n', 'vn', 'n', 'd', 'v', 'n', 'vn', 'u', 'n', 'w', 'v', 'ns', 'n', 'i', 'w', 'p', 'n', 'n', 'v', 'w', 'v', 'a', 'n', 'w', 'ad', 'v', 'ns', 'n', 'p', 'n', 'n', 'vn', 'u', 'ad', 'v', 'w', 'ad', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'n', 'v', 'u', 'n', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['nt', 'w', 'nt', 'vn', 'n', 'w', 'n', 'n', 'c', 'ns', 'vn', 'vn', 'n', 'w', 'p', 'j', 'n', 'w', 'n', 'n', 'd', 'm', 'n', 'v', 'u', 'n', 'w']\n",
      "['nt', 'w', 'nt', 'vn', 'n', 'w', 'n', 'n', 'c', 'ns', 'vn', 'vn', 'n', 'w', 'p', 'j', 'n', 'w', 'v', 'n', 'd', 'm', 'n', 'v', 'u', 'n', 'w']\n",
      "['n', 'p', 'j', 'w', 'j', 'w', 'ns', 'an', 'vn', 'n', 'w', 'ns', 'n', 'vn', 'n', 'w', 'n', 'j', 'j', 'vn', 'n', 'w', 'nz', 'n', 'ns', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'd', 'v', 'w']\n",
      "['n', 'p', 'n', 'w', 'n', 'w', 'ns', 'a', 'vn', 'n', 'w', 'ns', 'n', 'vn', 'n', 'w', 'n', 'j', 'j', 'vn', 'n', 'w', 'nz', 'n', 'ns', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'd', 'v', 'w']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'v', 'n', 'nr', 'nr', 'u', 'v']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'vn', 'n', 'nr', 'nr', 'u', 'v']\n",
      "93948/101136=0.9289273849074513\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging v1.1: add sentences containing unknown words \n",
    "\n",
    "def is_known_word(w, all_tags):\n",
    "    return len(list(\n",
    "            filter(lambda s: s != -float('inf'), [prob_word_by_tag[t][w] for t in all_tags])\n",
    "        )) > 0\n",
    "\n",
    "\n",
    "def tagging(words, all_tags):\n",
    "    N = len(words)\n",
    "    mat = list()\n",
    "    for _ in range(N):\n",
    "        mat.append(defaultdict(float))\n",
    "        \n",
    "    back_trace = defaultdict(list)\n",
    "    isknownword = is_known_word(words[0], all_tags)\n",
    "    for t in all_tags:\n",
    "        if isknownword:\n",
    "            mat[0][t] = prob_word_by_tag[t][words[0]]\n",
    "        else:\n",
    "            mat[0][t] = 0.0\n",
    "        back_trace[t] = [t]\n",
    "        \n",
    "    for idx,w in enumerate(words[1:]):\n",
    "        idx += 1\n",
    "        new_back_trace = defaultdict(list)\n",
    "        \n",
    "        isknownword = is_known_word(w, all_tags)\n",
    "        for t in all_tags:\n",
    "            if isknownword:\n",
    "                score, pt = max((s+prob_word_by_tag[t][w]+prob_tag_by_tag[pt][t],pt) for pt,s in mat[idx-1].items())\n",
    "                mat[idx][t] = score\n",
    "                new_back_trace[t] = back_trace[pt] + [t]\n",
    "            else:\n",
    "                # for unknown words, use only the best subsequent tag following tag_prev\n",
    "                score, pt = max((s+prob_tag_by_tag[pt][t],pt) for pt,s in mat[idx-1].items())    \n",
    "                mat[idx][t] = score\n",
    "                new_back_trace[t] = back_trace[pt] + [t]\n",
    "        back_trace = new_back_trace\n",
    "        \n",
    "    max_score, final_tag = max((s,t) for t,s in mat[N-1].items())\n",
    "    \n",
    "    return max_score, back_trace[final_tag]\n",
    "    \n",
    "\n",
    "num_matched_total = 0\n",
    "num_total = 0\n",
    "\n",
    "cnt=10\n",
    "for line in test_set:\n",
    "    tokens = line.split()[1:] # ignore 0th timestamp\n",
    "    if not tokens:\n",
    "        continue\n",
    "    words, tags = zip(*[tk.split('/') for tk in tokens])\n",
    "    tags = list(map(lambda t: t.split(']')[0] if t.find(']')>=0 else t, tags)) # case: 大会堂/n]ns, ignore the sub-tag\n",
    "    assert len(words) == len(tags)\n",
    "            \n",
    "    max_score, tags_predict = tagging(words, prob_tag_by_tag.keys())\n",
    "    if cnt>0:\n",
    "        print(tags)\n",
    "        cnt -= 1\n",
    "        print(tags_predict)\n",
    "    assert len(tags) == len(tags_predict)\n",
    "    \n",
    "    num_matched_total += len(list(filter(lambda x: x, [t1==t2 for t1,t2 in zip(tags,tags_predict)])))\n",
    "    num_total += len(tags)\n",
    "\n",
    "print('{0}/{1}={2}'.format(num_matched_total, num_total, float(num_matched_total)/num_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['w', 'nx', 'w', 'nx', 'w']\n",
      "['r', 'v', 'ns', 'n', 'i', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'v', 'n', 'a', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'u', 'n', 'n', 'w', 'ad', 'v', 'ns', 'n', 'v', 'u', 'n', 'w', 'c', 'c', 'n', 'w', 'j', 'n', 'w', 'c', 'n', 'w', 'n', 'w', 'n', 'n', 'w', 'n', 'w', 'c', 'c', 'v', 'p', 'ns', 'u', 'r', 'n', 'n', 'c', 'f', 'n', 'n', 'w', 'c', 'v', 'p', 'ns', 'w', 'ns', 'c', 's', 'u', 'ns', 'n', 'w', 'c', 'c', 'v', 'v', 'n', 'n', 'c', 'n', 'vn', 'u', 'n', 'w', 'c', 'v', 'v', 'w', 'v', 'c', 'n', 'n', 'd', 'v', 'a', 'w', 'vn', 'n', 'u', 'n', 'w', 'r', 'd', 'd', 'p', 'r', 'ad', 'v', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 't', 'p', 'w', 'j', 'w', 'u', 'n', 'v', 'n', 'n', 'vn', 'w', 'p', 'ns', 'n', 'u', 'a', 'n', 'c', 'a', 'n', 'v', 'r', 'vn', 'w', 'v', 'p', 'ns', 'p', 'w', 'j', 'w', 'u', 'n', 'f', 'v', 'an', 'an', 'w', 'd', 'a', 'u', 'ns', 'n', 'd', 'd', 'v', 'p', 'w', 'j', 'w', 'u', 'vn', 'c', 'vn', 'w']\n",
      "['r', 'v', 'ns', 'n', 'i', 'u', 'n', 'w', 'vn', 'c', 'v', 'ns', 'n', 'vn', 'n', 'a', 'u', 'n', 'w', 'v', 'c', 'v', 'ns', 'n', 'u', 'n', 'n', 'w', 'ad', 'v', 'ns', 'n', 'v', 'u', 'n', 'w', 'c', 'v', 'n', 'w', 'j', 'n', 'w', 'c', 'n', 'w', 'n', 'w', 'n', 'n', 'w', 'n', 'w', 'c', 'v', 'v', 'p', 'ns', 'u', 'r', 'v', 'n', 'c', 'f', 'w', 'n', 'w', 'c', 'v', 'p', 'ns', 'w', 'ns', 'c', 's', 'u', 'ns', 'n', 'w', 'c', 'v', 'n', 'vn', 'n', 'n', 'c', 'n', 'v', 'u', 'n', 'w', 'c', 'v', 'v', 'w', 'vn', 'c', 'n', 'n', 'd', 'v', 'a', 'w', 'vn', 'n', 'u', 'n', 'w', 'r', 'd', 'd', 'p', 'r', 'ad', 'v', 'w', 'v', 'vn', 'w', 'v', 'v', 'w', 'v', 'v', 'w', 't', 'p', 'w', 'j', 'w', 'u', 'n', 'v', 'n', 'n', 'vn', 'w', 'p', 'ns', 'n', 'u', 'n', 'n', 'c', 'a', 'n', 'v', 'r', 'v', 'w', 'v', 'p', 'ns', 'p', 'w', 'j', 'w', 'u', 'n', 'f', 'v', 'an', 'an', 'w', 'd', 'a', 'u', 'ns', 'n', 'd', 'd', 'v', 'p', 'w', 'j', 'w', 'u', 'vn', 'c', 'vn', 'w']\n",
      "['nr', 'nr', 'f', 'v', 'w', 'n', 'n', 'vn', 'v', 'v', 'v', 'u', 'n', 'w', 'ns', 'n', 'd', 'v', 'v', 'vn', 'w', 'n', 'vn', 'd', 'v', 'v', 'w', 'r', 'p', 'r', 'l', 'f', 'w', 'p', 'b', 'ns', 'n', 'l', 'w']\n",
      "['nr', 'nr', 'f', 'v', 'w', 'n', 'n', 'v', 'v', 'v', 'v', 'u', 'n', 'w', 'ns', 'n', 'd', 'v', 'v', 'v', 'w', 'n', 'vn', 'd', 'v', 'v', 'w', 'r', 'p', 'r', 'l', 'f', 'w', 'p', 'b', 'ns', 'n', 'n', 'w']\n",
      "['nt', 'ns', 'vn', 'n', 'w', 'nt', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'p', 'vn', 'f', 'v', 'w', 'nr', 'n', 'p', 'a', 'n', 'f', 'v', 'c', 'p', 'j', 'n', 'f', 'v', 'u', 'p', 'v', 'w', 'p', 'm', 'ns', 'u', 'n', 'f', 'w', 'ad', 'v', 'n', 'a', 'n', 'w', 'vn', 'u', 'a', 'n', 'w', 'ad', 'v', 'u', 'r', 'n', 'c', 'n', 'v', 'n', 'n', 'vn', 'u', 'n', 'w', 'v', 'n', 'n', 'u', 'a', 'vn', 'w', 'c', 'ns', 'n', 'ad', 'v', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'vn', 'u', 'n', 'w', 'nt', 'j', 'w', 'nt', 'j', 'd', 'v', 'v', 'n', 'n', 'n', 'n', 'p', 'n', 'v', 'n', 'vn', 'u', 'n', 'vn', 'w', 'v', 'ns', 'n', 'ad', 'v', 'r', 'u', 'n', 'c', 'n', 'w', 'd', 'v', 'u', 'd', 'p', 'r', 'v', 'n', 'vn', 'w', 'p', 'v', 'n', 'n', 'v', 'a', 'u', 'vn', 'w']\n",
      "['nt', 'ns', 'vn', 'n', 'w', 'nt', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'p', 'vn', 'f', 'v', 'w', 'nr', 'n', 'p', 'a', 'n', 'f', 'v', 'c', 'p', 'j', 'n', 'f', 'v', 'u', 'p', 'v', 'w', 'p', 'm', 'ns', 'u', 'n', 'f', 'w', 'ad', 'v', 'n', 'a', 'n', 'w', 'v', 'u', 'a', 'n', 'w', 'ad', 'v', 'u', 'r', 'n', 'c', 'n', 'v', 'n', 'n', 'vn', 'u', 'n', 'w', 'v', 'n', 'n', 'u', 'a', 'vn', 'w', 'c', 'ns', 'n', 'ad', 'v', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'vn', 'u', 'n', 'w', 'nt', 'j', 'w', 'nt', 'j', 'd', 'v', 'v', 'n', 'n', 'n', 'n', 'c', 'v', 'v', 'n', 'vn', 'u', 'n', 'vn', 'w', 'v', 'ns', 'n', 'ad', 'v', 'r', 'u', 'n', 'c', 'n', 'w', 'd', 'v', 'u', 'd', 'p', 'r', 'v', 'n', 'vn', 'w', 'p', 'v', 'n', 'n', 'v', 'a', 'u', 'vn', 'w']\n",
      "['ns', 'an', 'n', 'n', 'nr', 'nr', 'v', 'r', 'n', 'w', 'n', 'j', 'c', 'l', 'w', 'nz', 'n', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'v', 'ns', 'an', 'vn', 'n', 'w', 'n', 'j', 'w', 'n', 'n', 'n', 'n', 'w', 'n', 'n', 'n', 'm', 'n', 'nr', 'nr', 'v', 'n', 'n', 'w', 'n', 'n', 'c', 'n', 'j', 'd', 'p', 't', 'v', 'w', 'r', 'v', 'w', 'nr', 'n', 'v', 'u', 'vn', 'n', 'n', 'w', 'v', 'n', 'n', 'vn', 'n', 'u', 'm', 'q', 'n', 'w', 'd', 'v', 'u', 'n', 'u', 'a', 'n', 'w', 'v', 'u', 's', 'ns', 'n', 'u', 'b', 'n', 'w', 'v', 'v', 'n', 'vn', 'u', 'n', 'c', 'n', 'd', 'v', 'u', 'l', 'u', 'n', 'w', 'r', 'n', 'w', 'n', 'j', 'w', 'r', 'l', 'c', 'l', 'ad', 'v', 'nr', 'n', 'vn', 'n', 'd', 'v', 'n', 'vn', 'u', 'n', 'w', 'v', 'ns', 'n', 'i', 'w', 'p', 'n', 'n', 'v', 'w', 'v', 'a', 'n', 'w', 'ad', 'v', 'ns', 'n', 'p', 'n', 'n', 'vn', 'u', 'a', 'vn', 'w', 'ad', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'n', 'vn', 'u', 'n', 'w']\n",
      "['ns', 'a', 'n', 'n', 'nr', 'nr', 'v', 'r', 'n', 'w', 'n', 'j', 'c', 'l', 'w', 'nz', 'n', 'ns', 'n', 'n', 'n', 'nr', 'nr', 'v', 'ns', 'a', 'vn', 'n', 'w', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'w', 'n', 'n', 'n', 'm', 'n', 'nr', 'nr', 'n', 'n', 'n', 'w', 'n', 'n', 'c', 'n', 'j', 'd', 'p', 't', 'v', 'w', 'r', 'v', 'w', 'nr', 'n', 'v', 'u', 'vn', 'n', 'n', 'w', 'v', 'n', 'n', 'vn', 'n', 'u', 'm', 'q', 'n', 'w', 'd', 'v', 'u', 'n', 'u', 'a', 'n', 'w', 'v', 'u', 's', 'ns', 'n', 'u', 'b', 'n', 'w', 'v', 'v', 'n', 'v', 'u', 'n', 'c', 'n', 'd', 'a', 'u', 'l', 'u', 'n', 'w', 'r', 'n', 'w', 'n', 'j', 'w', 'r', 'l', 'c', 'l', 'ad', 'v', 'nr', 'n', 'vn', 'n', 'd', 'v', 'n', 'vn', 'u', 'n', 'w', 'v', 'ns', 'n', 'i', 'w', 'p', 'n', 'n', 'v', 'w', 'v', 'a', 'n', 'w', 'ad', 'v', 'ns', 'n', 'p', 'n', 'n', 'vn', 'u', 'ad', 'v', 'w', 'ad', 'v', 'v', 'n', 'n', 'vn', 'c', 'n', 'n', 'v', 'u', 'n', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['v', 'n', 'u', 'v', 'w', 'n', 'j', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'n', 'j', 'b', 'n', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w', 'nr', 'nr', 'w']\n",
      "['nt', 'w', 'nt', 'vn', 'n', 'w', 'n', 'n', 'c', 'ns', 'vn', 'vn', 'n', 'w', 'p', 'j', 'n', 'w', 'n', 'n', 'd', 'm', 'n', 'v', 'u', 'n', 'w']\n",
      "['nt', 'w', 'nt', 'vn', 'n', 'w', 'n', 'n', 'c', 'ns', 'vn', 'vn', 'n', 'w', 'p', 'j', 'n', 'w', 'v', 'n', 'd', 'm', 'n', 'v', 'u', 'n', 'w']\n",
      "['n', 'p', 'j', 'w', 'j', 'w', 'ns', 'an', 'vn', 'n', 'w', 'ns', 'n', 'vn', 'n', 'w', 'n', 'j', 'j', 'vn', 'n', 'w', 'nz', 'n', 'ns', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'd', 'v', 'w']\n",
      "['n', 'p', 'n', 'w', 'n', 'w', 'ns', 'a', 'vn', 'n', 'w', 'ns', 'n', 'vn', 'n', 'w', 'n', 'j', 'j', 'vn', 'n', 'w', 'nz', 'n', 'ns', 'n', 'n', 'w', 'n', 'n', 'n', 'n', 'd', 'v', 'w']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nt', 'n', 'v', 't', 'n']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'v', 'n', 'nr', 'nr', 'u', 'v']\n",
      "['nr', 'nr', 'p', 'nt', 'n', 'vn', 'n', 'vn', 'n', 'nr', 'nr', 'u', 'v']\n",
      "94016/101136=0.9295997468754944\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging v1.2: p(t|t_prev) in start or end place in a sentence\n",
    "\n",
    "def is_known_word(w, all_tags):\n",
    "    return len(list(\n",
    "            filter(lambda s: s != -float('inf'), [prob_word_by_tag[t][w] for t in all_tags])\n",
    "        )) > 0\n",
    "\n",
    "\n",
    "def tagging(words, all_tags):\n",
    "    N = len(words)\n",
    "    mat = list()\n",
    "    for _ in range(N):\n",
    "        mat.append(defaultdict(float))\n",
    "        \n",
    "    back_trace = defaultdict(list)\n",
    "    isknownword = is_known_word(words[0], all_tags)\n",
    "    for t in all_tags:\n",
    "        # adding start symbol\n",
    "        if isknownword:\n",
    "            mat[0][t] = prob_tag_by_tag['^'][t] + prob_word_by_tag[t][words[0]]\n",
    "        else:\n",
    "            mat[0][t] = prob_tag_by_tag['^'][t]\n",
    "        back_trace[t] = [t]\n",
    "        \n",
    "    for idx,w in enumerate(words[1:]):\n",
    "        idx += 1\n",
    "        new_back_trace = defaultdict(list)\n",
    "        \n",
    "        isknownword = is_known_word(w, all_tags)\n",
    "        for t in all_tags:\n",
    "            if isknownword:\n",
    "                score, pt = max((s+prob_word_by_tag[t][w]+prob_tag_by_tag[pt][t],pt) for pt,s in mat[idx-1].items())\n",
    "                mat[idx][t] = score\n",
    "                new_back_trace[t] = back_trace[pt] + [t]\n",
    "            else:\n",
    "                # for unknown words, use only the best subsequent tag following tag_prev\n",
    "                score, pt = max((s+prob_tag_by_tag[pt][t],pt) for pt,s in mat[idx-1].items())    \n",
    "                mat[idx][t] = score\n",
    "                new_back_trace[t] = back_trace[pt] + [t]\n",
    "        back_trace = new_back_trace\n",
    "        \n",
    "    # adding end symbol\n",
    "    max_score, final_tag = max((s + prob_tag_by_tag[t]['$'],t) for t,s in mat[N-1].items())\n",
    "    \n",
    "    return max_score, back_trace[final_tag]\n",
    "    \n",
    "\n",
    "############## TRAIN ##############\n",
    "f = '199801/199801.txt'\n",
    "with open(f, 'rb') as fin:\n",
    "    all = fin.read().decode('gbk').split('\\n')\n",
    "\n",
    "fold = 0.9\n",
    "train_len = int(len(all) * fold)\n",
    "train_set = all[:train_len]\n",
    "test_set = all[train_len:]\n",
    "\n",
    "from collections import defaultdict # matrix as incursive dict\n",
    "prob_word_by_tag = dict() # emit matrix\n",
    "prob_tag_by_tag = dict()  # tran matrix\n",
    "\n",
    "prob_word_by_tag['^'] = defaultdict(lambda: -float('inf')) # no use, just place holder\n",
    "\n",
    "for line in train_set:\n",
    "    tokens = line.split()[1:] # ignore 0th timestamp\n",
    "    prev_tag = '^' # start symbol\n",
    "    for token in tokens:\n",
    "        if token.strip() == '':\n",
    "            continue\n",
    "            \n",
    "        word, tag = token.split('/')\n",
    "        # case: 大会堂/n]ns\n",
    "        if tag.find(']') != -1:\n",
    "            tag = tag.split(']')[0] # ignore the sub-tag\n",
    "        \n",
    "        if not prob_word_by_tag.get(tag, None):\n",
    "            prob_word_by_tag[tag] = defaultdict(lambda: -float('inf'))\n",
    "        prob_word_by_tag[tag][word] = prob_word_by_tag[tag].get(word, 0) + 1\n",
    "        \n",
    "        if not prob_tag_by_tag.get(prev_tag, None):\n",
    "            prob_tag_by_tag[prev_tag] = defaultdict(lambda: -float('inf'))\n",
    "        prob_tag_by_tag[prev_tag][tag] = prob_tag_by_tag[prev_tag].get(tag, 0) + 1\n",
    "        prev_tag = tag\n",
    "        \n",
    "    # end symbol\n",
    "    if not prob_tag_by_tag.get(prev_tag, None):\n",
    "            prob_tag_by_tag[prev_tag] = defaultdict(lambda: -float('inf'))\n",
    "    prob_tag_by_tag[prev_tag]['$'] = prob_tag_by_tag[prev_tag].get('$', 0) + 1\n",
    "\n",
    "\n",
    "# normalize\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# use log instead of original probability, to avoid the chain product of probs converges into zero\n",
    "for tag,words_cnt in prob_word_by_tag.items():\n",
    "    ntotal = sum([c for w,c in words_cnt.items()])\n",
    "    for w,c in words_cnt.items():\n",
    "        words_cnt[w] = math.log(float(c) / ntotal)\n",
    "for prev_tag,tags_cnt in prob_tag_by_tag.items():\n",
    "    ntotal = sum([c for t,c in tags_cnt.items()])\n",
    "    for t,c in tags_cnt.items():\n",
    "        tags_cnt[t] = math.log(float(c) / ntotal)\n",
    "    \n",
    "    \n",
    "############## TEST ##############\n",
    "num_matched_total = 0\n",
    "num_total = 0\n",
    "\n",
    "cnt=10\n",
    "for line in test_set:\n",
    "    tokens = line.split()[1:] # ignore 0th timestamp\n",
    "    if not tokens:\n",
    "        continue\n",
    "    words, tags = zip(*[tk.split('/') for tk in tokens])\n",
    "    tags = list(map(lambda t: t.split(']')[0] if t.find(']')>=0 else t, tags)) # case: 大会堂/n]ns, ignore the sub-tag\n",
    "    assert len(words) == len(tags)\n",
    "            \n",
    "    max_score, tags_predict = tagging(words, prob_tag_by_tag.keys())\n",
    "    if cnt>0:\n",
    "        print(tags)\n",
    "        cnt -= 1\n",
    "        print(tags_predict)\n",
    "    assert len(tags) == len(tags_predict)\n",
    "    \n",
    "    num_matched_total += len(list(filter(lambda x: x, [t1==t2 for t1,t2 in zip(tags,tags_predict)])))\n",
    "    num_total += len(tags)\n",
    "\n",
    "print('{0}/{1}={2}'.format(num_matched_total, num_total, float(num_matched_total)/num_total))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
